# frontend settings
minWorkers: 1
maxWorkers: 1
maxBatchDelay: 4000
responseTimeout: 5200
batchSize: 4
deviceType: "gpu"

system:
  extraFiles: "test.tar.gz"
  maxGPUMemory: 500 # Max GPU Memory needed by model in MB

# deviceType: cpu # cpu, gpu, neuron
# deviceIds: [0,1,2,3] # gpu device ids allocated to this model.
# parallelType: "pp" # options depending on the solution, pp(pipeline parallelism), tp(tensor parallelism), pptp ( pipeline and tensor parallelism)
                   # This will be used to route input to either rank0 or all ranks from fontend based on the solution (e.g. DeepSpeed support tp, PiPPy support pp)
# torchrun:
#     nproc-per-node: 4 # specifies the number of processes torchrun starts to serve your model, set to world_size or number of
                      # gpus you wish to split your model
#backend settings
# pippy:
#     chunks: 1 # This sets the microbatch sizes, microbatch = batch size/ chunks
#     input_names: ['input_ids'] # input arg names to the model, this is required for FX tracing
#     model_type: "HF" # set the model type to HF if you are using Huggingface model other wise leave it blank or any other model you use.
#     rpc_timeout: 1800
#     num_worker_threads: 512 #set number of threads for rpc worker init.

# handler:
#     max_length: 80 # max length of tokens for tokenizer in the handler